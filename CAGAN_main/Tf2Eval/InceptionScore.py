# -*- coding: utf-8 -*-
"""TEST_IS_FID.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hgJJI5wuILxcHsmrkZMkHJtk6uDlKOwr
"""

# This Colab notebook first calculates the Inception Score of the training set of CIFAR-10,
# then calculates the Fréchet Inception Distance between the training set and test set of CIFAR-10,
# base on https://github.com/tsc2017/Inception-Score and https://github.com/tsc2017/Frechet-Inception-Distance

DATA_DIR = './datasets/cifar10/cifar-10-batches-py'
HEIGHT = WIDTH = 32
DATA_DIM = HEIGHT * WIDTH * 3
BATCH_SIZE = 50

import tensorflow as tf
import urllib
import gzip
import pickle
import tensorflow.compat.v1 as tf

tf.disable_v2_behavior()
import os
import functools
import numpy as np
import time
from tensorflow.python.ops import array_ops
import tensorflow_gan as tfgan

import search_cfg
args = search_cfg.parse_args()
# set visible GPU ids
os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_ids     # GPU设定 要在初始化session之前

session = tf.compat.v1.InteractiveSession()
# A smaller BATCH_SIZE reduces GPU memory usage, but at the cost of a slight slowdown
BATCH_SIZE = 50
# INCEPTION_TFHUB = 'https://tfhub.dev/tensorflow/tfgan/eval/inception/1'
INCEPTION_TFHUB = './Tf2Eval/tfgan_eval_inception_1'
INCEPTION_OUTPUT = 'logits'


def unpickle(file):
    fo = open(file, 'rb')
    dict = pickle.load(fo, encoding='latin1')
    fo.close()
    return dict['data'], dict['labels']


def cifar_generator(filenames, batch_size, data_dir):
    all_data = []
    all_labels = []
    for filename in filenames:
        data, labels = unpickle(data_dir + '/' + filename)
        all_data.append(data)
        all_labels.append(labels)

    images = np.concatenate(all_data, axis=0).reshape([-1, 3, 32, 32]).transpose([0, 2, 3, 1]).reshape(
        [-1, 32 * 32 * 3])
    labels = np.concatenate(all_labels, axis=0)

    def get_epoch():
        rng_state = np.random.get_state()
        np.random.shuffle(images)
        np.random.set_state(rng_state)
        np.random.shuffle(labels)

        for i in range(len(images) // batch_size):
            yield (images[i * batch_size:(i + 1) * batch_size], labels[i * batch_size:(i + 1) * batch_size])

    return get_epoch


def load(batch_size, data_dir):
    return (
        cifar_generator(['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5'], batch_size,
                        data_dir),
        cifar_generator(['test_batch'], batch_size, data_dir)
    )


def inf_gen(MODE='TRAIN', BATCH_SIZE=BATCH_SIZE):
    if MODE == 'TRAIN':
        train_gen, _ = load(BATCH_SIZE, data_dir=DATA_DIR)
        while True:
            for original_images, labels in train_gen():
                yield 2. / 255 * original_images - 1, labels
    elif MODE == 'TEST':
        _, test_gen = load(BATCH_SIZE, data_dir=DATA_DIR)
        while True:
            for original_images, labels in test_gen():
                yield 2. / 255 * original_images - 1, labels


train_gen = inf_gen('TRAIN')
test_gen = inf_gen('TEST')

'''
From https://github.com/tsc2017/Inception-Score
Code derived from https://github.com/openai/improved-gan/blob/master/inception_score/model.py and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py

Usage:
    Call get_inception_score(images, splits=10)
Args:
    images: A numpy array with values ranging from 0 to 255 and shape in the form [N, 3, HEIGHT, WIDTH] where N, HEIGHT and WIDTH can be arbitrary. A dtype of np.uint8 is recommended to save CPU memory.
    splits: The number of splits of the images, default is 10.
Returns:
    Mean and standard deviation of the Inception Score across the splits.
'''

# Run images through Inception.
inception_images = tf.compat.v1.placeholder(tf.float32, [None, 3, None, None], name='inception_images')


def inception_logits(images=inception_images, num_splits=1):
    images = tf.transpose(images, [0, 2, 3, 1])
    size = 299
    images = tf.compat.v1.image.resize_bilinear(images, [size, size])
    generated_images_list = array_ops.split(images, num_or_size_splits=num_splits)
    logits = tf.map_fn(
        fn=tfgan.eval.classifier_fn_from_tfhub(INCEPTION_TFHUB, INCEPTION_OUTPUT, True),
        elems=array_ops.stack(generated_images_list),
        parallel_iterations=8,
        back_prop=False,
        swap_memory=True,
        name='RunClassifier')
    logits = array_ops.concat(array_ops.unstack(logits), 0)
    return logits


logits = inception_logits()


def get_inception_probs(inps):
    session = tf.get_default_session()
    n_batches = int(np.ceil(float(inps.shape[0]) / BATCH_SIZE))
    preds = np.zeros([inps.shape[0], 1000], dtype=np.float32)
    for i in range(n_batches):
        inp = inps[i * BATCH_SIZE:(i + 1) * BATCH_SIZE] / 255. * 2 - 1
        preds[i * BATCH_SIZE: i * BATCH_SIZE + min(BATCH_SIZE, inp.shape[0])] = session.run(logits,
                                                                                            {inception_images: inp})[:,
                                                                                :1000]
    preds = np.exp(preds) / np.sum(np.exp(preds), 1, keepdims=True)
    return preds


def preds2score(preds, splits=10):
    scores = []
    for i in range(splits):
        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]
        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))
        kl = np.mean(np.sum(kl, 1))
        scores.append(np.exp(kl))
    return np.mean(scores), np.std(scores)


def get_inception_score(images, splits=10):

    images = np.array(images) # (5000, 32, 32, 3)
    images = np.transpose(images, [0, 3, 1, 2])    # change type + transpose is about 12.7 s

    assert (type(images) == np.ndarray)
    assert (len(images.shape) == 4)
    assert (images.shape[1] == 3)
    assert (np.min(images[0]) >= 0 and np.max(images[0]) > 10),   'Image values should be in the range [0, 255]'
    print('Calculating Inception Score with %i images in %i splits' % (images.shape[0], splits))   # (50000, 3, 32, 32)
    start_time = time.time()
    preds = get_inception_probs(images)
    mean, std = preds2score(preds, splits)
    print('Inception Score calculation time: %f s' % (time.time() - start_time))
    return mean, std    # Reference values: 11.38 for 50000 CIFAR-10 training set images, or mean=11.31, std=0.10 if in 10 splits.


# this is the most importance function

# def get_training_set_is(n=50000, splits=10):
#     all_samples = np.zeros([int(np.ceil(float(n) / BATCH_SIZE) * BATCH_SIZE), DATA_DIM], dtype=np.uint8)
#     for i in range(int(np.ceil(float(n) / BATCH_SIZE))):  # inception score for num_batches of real data
#         all_samples[i * BATCH_SIZE:(i + 1) * BATCH_SIZE] = ((next(train_gen)[0] + 1) / 2 * 255).astype(np.uint8)
#     return get_inception_score(all_samples[:n].reshape([-1, HEIGHT, WIDTH, 3]).transpose([0, 3, 1, 2]), splits)
#
#
#
# # Inception Score of the training set in 10 splits
# is_mean_and_std = get_training_set_is()
# print('Inception Score: mean=%f, std=%f' % is_mean_and_std)

